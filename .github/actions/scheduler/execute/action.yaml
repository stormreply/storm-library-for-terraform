name: Execute

runs:
  using: composite
  steps:

    - name: Execute
      shell: bash
      run: |
        # scheduler/execute

        for e in $(cat /tmp/selected) ; do

          file=${e##*/} ; f=${file}
          date=${f%%-*} ; f=${f##$date-}
          time=${f%%-*} ; f=${f##$time-}
          action=${f%%-*} ; f=${f##$action-}
          account_id=${f%%-*} ; f=${f##$account_id-}
          region=${f%-${f#*-*-*-}} ; f=${f##$region-}
          slt=${f%%-*} ; f=${f##$slt-}
          catalog_id=${f%%-*} ; f=${f##$catalog_id-}
          repo=${f%-*}
          environment=${f##$repo-}

          printf "%6s %6s %7s %12s %15s %s %s %s %s\n" \
            "DATE" "TIME" "ACTION" "ACCOUNT_ID" "REGION" \
            "SLT" "CATALOG_ID" "REPO" "ENVIRONMENT"
          printf "%6s %6s %7s %12s %15s %s %s %s %s\n" \
            "$date" "$time" "$action" "$account_id" "$region" \
            "$slt" "$catalog_id" "$repo" "$environment"

          aws s3 cp \
            s3://${{ env.BACKEND_BUCKET }}/schedule/$file \
            /tmp \
            --quiet

          cat << EOF
          EXECUTE
            cat /tmp/$file
            | gh workflow run ${action}.yaml --repo stormreply/${repo} --json
        EOF

          # TODO: handle "stormreply": put into yaml in s3 object itself, together with tfvars name
          cat /tmp/$file \
          | gh workflow run ${action}.yaml \
            --repo stormreply/${repo} \
            --json \
            &

        done
