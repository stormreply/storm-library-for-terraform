name: Execute

runs:
  using: composite
  steps:

    - name: Execute
      shell: bash
      run: |
        # scheduler/execute

        for job in $(cat /tmp/selected) ; do

          file=${job##*/} ; f=${file}
          date=${f%%.*} ; f=${f##$date.}
          time=${f%%.*} ; f=${f##$time.}
          action=${f%%.*} ; f=${f##$action.}
          account_id=${f%%.*} ; f=${f##$account_id.}
          region=${f%%.*} ; f=${f##$region.}
          repo=${f%%.*} ; environment=${f##$repo.}

          printf "%-8s %-6s %-7s %-12s %-15s %-s %-s\n" \
            "DATE" "TIME" "ACTION" "ACCOUNT_ID" "REGION" "REPO" "ENVIRONMENT"
          printf "%-8s %-6s %-7s %-12s %-15s %-s %-s\n" \
            "$date" "$time" "$action" "$account_id" "$region" "$repo" "$environment"

          aws s3 cp \
            s3://${{ env.BACKEND_BUCKET }}/schedule/$file \
            /tmp \
            --quiet

          cat << EOF
          EXECUTE
            cat /tmp/$file
            | gh workflow run ${action}.yaml --repo stormreply/${repo} --json
        EOF

          # TODO: handle "stormreply": put into yaml in s3 object itself, together with tfvars name
          cat /tmp/$file \
          | gh workflow run ${action}.yaml \
            --repo stormreply/${repo} \
            --json \
            &

        done
